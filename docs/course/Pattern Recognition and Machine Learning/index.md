# 模式识别与机器学习

这是中国科学院大学人工智能学院本科生课程《模式识别与机器学习》的考试重点。笔者该门课程成绩为97/100。

## 重点

1. 模式识别概念、系统构成、方法种类（统计、生成式……）、类型（无监督、半监督……）

2. 如何评估一个系统：查全率、查准率、曲线……

3. 验证方法：留一法、交叉验证……

4. 曲线拟合：多项式拟合、线性回归

5. FLDA

6. 感知机

7. logistic回归

8. 二值交叉熵

9. 支持向量机

10. BP反传

11. 核方法

12. 贝叶斯、最大似然、正态分布条件下的贝叶斯

13. 概率密度函数的估计：参数化方法（最大似然估计、贝叶斯估计）、非参数化(parthon窗、k近邻)

14. 决策树（信息增益、基尼模型）

15. 概率图模型（了解，不考）

16. 集成学习基本思想和概念

17. k-means:EM思想

18. PCA重要

19. 卷积神经网络（重点）（auto encoder）、循环神经网络



## 考试详细重点
- 什么是模式识别
    - 模式（Pattern）：存在于时间与空间中具有**可观测性**、**可度量性**和**可区分性**的信息。
    - 识别（Recognition）：对模式进行分析与处理，进而实现描述、辨识、分类与解释。

- 系统
    - 系统的定义：
        - 系统：系统是由相互作用相互依赖的若干组成部分结合而成的，具有特定功能的有机整体，而且这个有机整体又是它从属的更大系统的组成部分。（钱学森）
        - 模式识别与机器学习系统：结合硬件与软件，对数据中存在的物体、行为、现象等模式通用规律进行检测、描述、判别与回归的系统，通常包括原始数据获取和预处理、特征提取、分类或决策、应用与部署等步骤。
    - 系统的构成（重点）：
        - 通常由数据获取、预处理、特征提取与选择、学习器设计与部署等环节构成
        - 部署前：数据获取->预处理->特征提取与选择->学习器设计->部署
        - 部署后：输入->预处理->特征提取与选择->决策->输出
        - 分类系统与回归系统
            - 分类系统：以K近邻法为例
                - 缺点：
                    - 储存空间要求高（需要存储每一个训练样本）
                    - 距离计算复杂（距离的选择、高位空间的计算）
                    - K值带来影响（K太小，方差大，因为对数据太敏感，容易被噪声影响。K太大，偏差大，因为更依赖周围点本身的分布）
                    - 需要对K进行调参
                - 优点：
                    - 理论简单，易于实现；
                    - 不需训练，支持在线学习；
                    - 准确性高、鲁棒性好；
                    - 天然支持多分类问题。
            - 回归系统： 
    - 系统的评估
        - 构建验证集：
            - 留出法（Hold-out）：直接将数据集划分为两个互斥集合
            - K折交叉验证（Cross-Validation）：分层划分为K个互斥子集
                - 有时会采用多种（M种）划分方式，每次求交叉验证结果，再求平均，可以称为M次K折交叉验证
                - 分层采样：将数据划分为K个大小相似的互斥子集，通过分层采样让每个子集的分布尽可能一致
                - 组合方式：每次用k-1个子集的并集作为训练集，余下的子集作为验证集，共得到K组划分
                - K的取值：测试结果的稳定性与保真性取决于K，也叫作“K折交叉验证”，K通常取10
            - 自助法（Bootstrap）：有放回采样（确保训练集大小）：对样本量为n的数据集每次有放回的随机采样得到n个样本作为训练集，未被采样到的数据作为测试集
                - 优点：可解决因训练样本规模导致的偏差，对小数据集、数据难以有效划分时很有效
                - 缺点：
                    - 存在数据浪费，约1/3样本未用于训练，连续n次不中的概率： $ \lim_{n \to \infty} (1-\frac{1}{n})^{n} = \frac{1}{e} = 0.368 $
                    - 有放回采样改变了原始数据分布，会引入估计偏差
                    - 所以初始数据量较多时，留出法、交叉验证法更为常用
        - 性能度量：
            - 回归任务：均方误差
            - 分类任务：
                - 混淆矩阵： 
                    - 标签为True，预测为Positive，称为TP(true positive)
                    - 标签为False，预测为Positive，称为FP(false positive)
                    - 标签为True，预测为Negative，称为FN（false negative，注意这里易错）
                    - 标签为False，预测为Positive，称为TN（true positive，注意这里易错）
                - 查准率precision $ P = \frac{TP}{TP+FP} $，查全率Recall $ R = \frac{TP}{TP+FN} $
                - P-R曲线：根据学习器的预测结果按正例可能性大小对样例进行排序，并逐个把样本作为正例进行预测，则可以得到查准率-查全率曲线，简称“P-R曲线”
                    - 平衡点 (BEP, Break-Even Point)是曲线上“查准率=查全率”时的取值，可用来用于度量P-R曲线有交叉的分类器性能高低
                - ROC曲线：以“假正例率FPR”为横轴，“真正例率TPR”

- 卷积神经网络
    - 网络正则化：
        - 提前停止： Early Stopping
        - 权重衰减： Weight Decay
        - 丢弃法： Dropout
        - 网络结构： Network Structure
    - CNN：
        - Zero padding：补零


- 决策树
    - 信息熵： $H(x)=-\sum_{i}p_{i}logp_{i}$ 
    - 信息增益：属性a对样本集D的信息增益 $Gain(D,A)=H(D)-H(D|A)$
    - 基尼值： $Gini(x)=1-\sum_{i}p_{i}^{2}$
    - 基尼指数：属性 a 各子结点的基尼值加权平均 $GiniIndex(D,a)=\sum_{v=1}^{V} \frac{|D^{v}|}{|D|}Gini(D^{v})$
    - ID3算法：比较所有的信息增益，选取最大的信息增益。如果后继结点只包含一类样本，就成为叶节点。
    - C4.5算法：使用信息增益率 $GainRatio(D,A)=\frac{Gian(D,A)}{IV(A)}$，其中 $IV(A)=-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}log\frac{|D^{v}|}{|D|}$
        - IV(A)相当于将子节点划分看做类别时的熵
        - 对可取值较少的属性的有偏好
        - 为了避免过度偏好，采取启发式策略：先从候选划分属性中选出信息增益高于平均值的，再从中选出信息增益率最高的
    - 剪枝：
        - 预剪枝：边划分边计算验证集精度，如果变差就不划分
            - 优点：降低过拟合风险、显著减少训练时间和测试时间开销
            - 缺点：基于贪心，可能欠拟合，因为后续可能会更优，但是被剪掉了
        - 后剪枝：生成完之后从叶节点开始一个一个去掉
            - 优点：后剪枝比预剪枝保留了更多分支，欠拟合风险小，泛化性能优于预剪枝决策树‘
            - 缺点：训练时间开销大

