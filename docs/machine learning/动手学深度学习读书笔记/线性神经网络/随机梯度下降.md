作用：在无法得到解析解的情况下，有效地训练模型
> 难以优化的模型效果往往更好

梯度下降：不断地在损失函数递减的方向上更新参数来降低误差

操作：计算损失函数关于模型参数的梯度

问题：执行速度慢，每次更新参数前需遍历整个数据集

解决：随机梯度下降

+ 思想：在每次需要计算更新时随机抽取一小批量样本
+ 步骤：
	1. 随机抽取小批量$B$（由固定数量的训练样本组成）
	2. 计算小批量的损失函数关于模型参数的梯度
	3. 当前参数减去梯度乘以学习率$\eta$
	4. 训练若干次后记录模型参数估计值$\hat w,\hat b$
$$
(\mathbf w,b)\leftarrow(\mathbf w,b)-\dfrac{\eta}{|B|}\sum_{i\in B}\partial_{\mathbf (w,b)}l^{(i)}(\mathbf w,b)
$$
其中$|B|$为批量大小

超参数：预先设定而非模型训练得到（如批量大小和学习率）

调参：选择超参数的过程，根据训练迭代结果（在验证数据集上评估得到）调整

难点：

+ 算法会使损失函数向最小值缓慢收敛，但不能在有限的步数内非常精确地达到最小值
+ 损失平面上通常包含多个极小值
+ 泛化：难以找到一组参数，在从未见过的数据上实现较小的损失